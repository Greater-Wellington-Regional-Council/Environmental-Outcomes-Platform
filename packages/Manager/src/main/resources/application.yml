spring:
  application:
    name: manager
  main:
    banner-mode: off
  datasource:
    url: jdbc:postgresql://${CONFIG_DATABASE_HOST:localhost}:${CONFIG_DATABASE_PORT:5432}/${CONFIG_DATABASE_NAME:eop_dev}
    username: ${CONFIG_DATABASE_USERNAME:eop_manager_app_user}
    password: ${CONFIG_DATABASE_PASSWORD:password}

  flyway:
    locations: classpath:/db/migration,classpath:/db/migration-dev
    user: ${CONFIG_DATABASE_MIGRATIONS_USERNAME:eop_manager_migrations_user}
    password: ${CONFIG_DATABASE_MIGRATIONS_PASSWORD:password}

  kafka:
    bootstrap-servers: localhost:29092

    consumer:
      auto-offset-reset: earliest
      key-deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
      properties:
        spring.json.value.type.method: nz.govt.eop.messages.KafkaMessageTypes.determineTypeFromTopicName
        spring.json.trusted.packages: nz.govt.eop.*
        spring.deserializer.key.delegate.class: org.apache.kafka.common.serialization.StringDeserializer
        spring.deserializer.value.delegate.class: org.springframework.kafka.support.serializer.JsonDeserializer


    # These producer settings are used when publishing messages to the DLQ
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      # In the situation that a message was received as invalid JSON and could not be deserialized, it ends up being a ByteArray message that gets published.
      # the JSON serializer will still serialize the ByteArray as a string, but it will be a string of bytes, not a string of characters.
      # See WaterAllocationConsumerErrorHandlerTest
      # Can't figure out a good way to avoid this without writing a custom serializer.
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      properties:
        spring.json.add.type.headers: false

    streams:
      properties:
        num.stream.threads: 4
        max.poll.records: 10
        default:
          key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
          value.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
      application-id: nz.govt.eop.consumers.manager


management:
  endpoints:
    enabled-by-default: true
    web:
      exposure:
        include: "*"

server:
  compression:
    enabled: true
    mime-types: application/json

logging:
  level:
    root: INFO
    nz.govt.eop: DEBUG
    org.jooq.Constants: OFF
    org.jooq: INFO
    org.apache.kafka: INFO
    org.apache.kafka.streams.processor.internals.assignment.RackAwareTaskAssignor: OFF # Because it logs an error at startup even though it isn't used.

